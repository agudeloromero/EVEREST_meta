cluster:
  mkdir -p logs/{rule} &&
  sbatch
	--cpus-per-task=5
	--mem=10000
	--job-name=smk-{rule}-{wildcards}
	--output=logs/{rule}/{rule}-{wildcards}-%j.out
#	--partition={resources.partition}
default-resources:
  - mem_mb=1000
restart-times: 3
max-jobs-per-second: 10
max-status-checks-per-second: 1
local-cores: 36 
latency-wait: 60
jobs: 100
keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
conda-frontend: mamba

#cluster:
#  mkdir -p logs/{rule} &&
#  sbatch
#	--qos={resources.qos}
#	--cpus-per-task={threads}
#	--mem={resources.mem_mb}
#	--job-name=smk-{rule}-{wildcards}
#	--output=logs/{rule}/{rule}-{wildcards}-%j.out
#	--partition={resources.partition}
#default-resources:
#  - partition=<name-of-default-partition>
#  - qos=<name-of-quality-of-service>
#  - mem_mb=1000
#restart-times: 3
#max-jobs-per-second: 10
#max-status-checks-per-second: 1
#local-cores: 36 
#latency-wait: 60
#jobs: 100
#keep-going: True
#rerun-incomplete: True
#printshellcmds: True
#scheduler: greedy
use-conda: True
conda-frontend: mamba
cluster-status: ~/.config/snakemake/slurm/slurm-status.py
